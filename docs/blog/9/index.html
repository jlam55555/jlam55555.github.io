<!DOCTYPE html><html><head><title>jlam | Scraping Twitter tweet contents by ID using Selenium</title><style>html {
	box-sizing: border-box;
	margin: 0;
	padding: 0.5rem;
}
body {
	max-width: 400pt;
	margin: 0 auto;
	padding: 0;
}</style><link rel="icon" href="/res/favicon.png"><meta name="viewport" content="width=device-width, initial-scale=1.0"></head><body><h1>Jonathan Lam</h1><p>EE/CS @ The Cooper Union</p><nav><a href="/">Home</a> | <a href="/contact">Contact</a> | <a href="/experience">Experience</a> | <a href="/res/resume.pdf" target="_blank">Resume</a> | <a href="http://files.lambdalambda.ninja">Files</a> | <a href="/blog">Blog</a></nav><hr><!-- inspiration taken from https://codepen.io/Zoxon/pen/WyqdBb--><h2>Blog</h2><h3>Scraping Twitter tweet contents by ID using Selenium</h3><p>On Sun Apr 25 2021 17:57:25 GMT-0400 (Eastern Daylight Time)</p><p><a href="/blog">Return to blog</a></p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/prism.min.js" integrity="sha512-YBk7HhgDZvBxmtOfUdvX0z8IH2d10Hp3aEygaMNhtF8fSOvBZ16D/1bXZTJV6ndk/L/DlXxYStP8jrF77v2MIg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-zc7WDnCM3aom2EziyDIRAtQg1mVXLdILE09Bo+aE1xk0AM2c2cVLfSW9NrxE5tKTX44WBY0Z2HClZ05ur9vB6A==" crossorigin="anonymous"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism-tomorrow.min.css" integrity="sha512-vswe+cgvic/XBoF1OcM/TeJ2FW0OofqAVdCZiEYkd6dwGXthvkSFWOoGGJgS2CW70VK5dQM5Oh+7ne47s74VTg==" crossorigin="anonymous"><style>pre {
	padding: .5rem !important;
	line-height: 1 !important;
}
code {
	line-height: 1 !important;
	font-size: 0.75rem !important;
	tab-size: 8 !important;
}
</style><p>It turns out that scraping Twitter tweets is not as simple as I had originally thought it to be. Twitter tweets seemed to me to be a pretty common source for NLP tasks, but the restrictions imposed by the platform make it slightly more annoying. Some of the troubles encountered were<sup id="footnote-1-indicator"><a href="#footnote-1">1</a></sup>:</p><ol><li>Firstly, there are some <a href="https://blog.ldodds.com/2017/05/19/can-you-publish-tweets-as-open-data/">restrictions on what data can be published about tweets</a>. Notably, loosely speaking, only tweet IDs can be stored in a public dataset, and not their contents. This makes the use of a Twitter API or scraping necessary for accessing tweets.</li><li>Secondly, a Twitter tweet URL's webpage is not static, but its contents are loaded dynamically using Javascript after the page load<sup id="footnote-2-indicator"><a href="#footnote-2">2</a></sup> Thus a simple HTML request and scrape (e.g., via BeautifulSoup) won't work. (The change seems to have happened circa December 2020.. You actually need to simulate a browser opening the page. Hence the use of Selenium webdriver, a Python-based<sup id="footnote-3-indicator"><a href="#footnote-3">3</a></sup> web-automation tool.</li><li>The obvious alternative to scraping Twitter webpages is to use their APIs, but this is rate-limited and require signing up for a developer account. This is a little more work and there is the possibility of Twitter rejecting your registration request or terminating your developer account if they suspect abuse of the API. (The rate limits are fairly good, but I misunderstood it from the documentation at first; this will be discussed later.)</li><li>The CSS classes are randomly generated, so it is not immediately clear what to scrape from the webpage.</li><li>There are a few libraries out there that scrape Twitter to avoid the API rate limits, but none of them seem to be able to grab tweet contents by ID. Rather, they seem to be able to grab tweets by a general search, by user ID, etc. I don't know what the cause of this is (nor do I want to dig through the dread that Python source code is), but it makes my particular use case extremely elusive. So I thought I'd do it myself.</li></ol><p>The first issue to address is the final one. By messing around in the Chrome DevTools, I was able to locate the tweet contents using the query:</p><pre><code class="language-css">[data-testid=tweet] ~ div > div
</code></pre><p>In words: the tweet (text) empirically seems to always be located inside a <code class="language-html">&lt;div&gt;</code>, which is the immediate child of another <code class="language-html">&lt;div&gt;</code> element that is the immediate sibling of an element with attribute <code class="language-css">data-testid="tweet"</code>. (This tag always seems to be contained within an <code class="language-html">&lt;article&gt;</code> element, but that part seems unnecessary for the selector to work.)</p><p>At first, I tried scraping a tweet by simply loading a request to the page and scraping for that element using BeautifulSoup, but as mentioned previously, this will not work because the Tweet contents are loaded dynamically after page load with Javascript.</p><p>So the general idea becomes: load the tweet webpage with a browser emulator like Selenium, and keep querying that selector until it is found (and up to a certain number of retries). This leads to the following ugly but working code. (This requires a working installation of Selenium and the corresponding browser driver.)</p><pre><code class="language-python"># tweets to scrape contents of
tweet_ids = [1298265396378656768, 1297273403175559173]

# selenium code: https://stackoverflow.com/a/53657649/2397327
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException

chrome_options = Options()
chrome_options.add_argument("--disable-extensions")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--headless")
driver = webdriver.Chrome('./chromedriver', options=chrome_options)

import time

# scrape
selector = '[data-testid=tweet]~div>div'
for tweet_id in tweet_ids:
    driver.get(f'https://twitter.com/u/status/{tweet_id}')

    tweet = ''
    retries = 0
    while tweet == '' and retries < 10:
        try:
            tweet = driver.find_element_by_css_selector(selector).text
        except NoSuchElementException:
            # element not loaded yet
            pass
        time.sleep(0.1)
        retries += 1

    print(f'Got tweet: {tweet}')

driver.quit()
</code></pre><p>In the end, this only scrapes a quote every 1-2 seconds. I tried parallelizing it with a few <code class="language-python">mp.Process</code> instances, each with their own webdriver, and a <code class="language-python">mp.Queue</code> to distribute the requests, but more than a few webdriver instances (~24) runs me out of RAM and a curious phenomenon causes many of the web requests to drop with more parallel instances. I'm not sure what the latter problem is caused by, but I guess it might be an issue with too many outgoing requests from a single network device, or perhaps some restriction by Twitter -- I'm not sure<sup id="footnote-4-indicator"><a href="#footnote-4">4</a></sup>.</p><p>Since that was ultimately a failure, I turned to tools using the API. The particular dataset I was using comprises approximately 300,000 tweets. Looking at the <a href="https://developer.twitter.com/en/docs/twitter-api/rate-limits">Twitter v2 API rate limits</a>, we see that there are only 900 requests per fifteen minutes (averaging one request per second), or 86400 per day. I thought that would mean 86,400 tweets per day, but the <a href="https://developer.twitter.com/en/docs/twitter-api/tweets/lookup/api-reference/get-tweets">GET /2/tweets</a> endpoint allows you to request up to 100 tweets per request, for a total of 8.64 million tweets per day.</p><p>So I signed up for a Twitter developer account<sup id="footnote-5-indicator"><a href="#footnote-5">5</a></sup>, and used the <a href="https://github.com/DocNow/hydrator">hydrator</a> Electron app to handle the API requests and rate-limiting to grab the tweets in less than an hour (three-and-some 15-minute spurts of 900 requests of 100 tweets each). The result was roughly 900MB of data, mostly metadata that I didn't need. Stripping down the data to only the tweet text content and IDs, it became 60MB of data.</p><p>In conclusion, the API turned out to be more than plenty<sup id="footnote-6-indicator"><a href="#footnote-6">6</a></sup>. I don't think the above code snippet will ever be useful, but it was fun learning how to use Selenium and touch upon scraping a valuable source of NLP data.</p><p id="footnote-1"><sup><a href="#footnote-1-indicator">1.</a> An additional apparent challenge is that the URL of a tweet to scrape contains both the username of the tweeter and the tweet ID; however, it turns out that tweet IDs are UUIDs and <a href="https://stackoverflow.com/a/30287049/2397327">putting any username in the correct spot will work</a>.</sup></p><p id="footnote-2"><sup><a href="#footnote-2-indicator">2.</a> This seems to be a (relatively) recent development. There are still many old scraping tools out there that were broken by this change, such as <a href="https://github.com/sebinsua/scrape-twitter">scrape-twitter (deprecated, NPM)</a>. More info <a href="https://stackoverflow.com/q/65403350/2397327">here</a>.</sup></p><p id="footnote-3"><sup><a href="#footnote-3-indicator">3.</a> I wouldn't usually use Python, but it was the weapon of choice for this particular project. I want to try out Puppeteer at some point, since it is JS-based and is supposed to be more performant.</sup></p><p id="footnote-4"><sup><a href="#footnote-4-indicator">4.</a> If this is truly the case, with some free time an expensive solution might be to spin up a good large fleet of AWS Lambdas or EC2 instances to distribute the requests so the requests wouldn't all be from the same host. This probably isn't too different from a DDoS attack.</sup></p><p id="footnote-5"><sup><a href="#footnote-5-indicator">5.</a> I signed up for an academic (student) version, which has the standard rate limits. It seems that academic researchers (post-grad) are allowed to get higher rate limits with proper verification.</sup></p><p id="footnote-6"><sup><a href="#footnote-6-indicator">6.</a> ... for this dataset; however, a related dataset to the one I'm using has over 1.2 billion tweets, which would take over four months to download at the API rate limit. Gladly, we're not using that dataset.</sup></p><hr><p>&copy; Copyright 2021 Jonathan Lam</p></body></html>