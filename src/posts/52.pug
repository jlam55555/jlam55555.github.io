extends blogpost

block post
  p I spent some time reviewing low-level computer-architecture and performance optimization topics recently, both to brush up on some of the topics from my work at Google, and for interview preparation as I continue my job search. Memory seems to be among the most pertinent of these.
  
  p For now I'll list some of the resources that I was reading, with a brief description of my learning. That being said, these are fantastic readings and I will probably take a long time to get a good handle on the topics they go into detail on, so I may not have a good grasp on them myself yet. I may also add documents later as I continue exploring this space.

  h4 Overview of memory in Linux
  p My exploration into memory in Linux began with this Drepper paper. It seems to be a highly recommended paper, also #[a(href="https://lwn.net/Articles/250967/") posted around the same time on LWN] and mentioned in the Morin article linked below. #[em This Drepper paper has instantly become one of my favorite papers of all time] -- I admire the in-depth technical theory, the practical experiments that demonstrate the theory and further understanding, and the clarity of speech (which seems to owe a lot to Jonathan Corbet, a regular poster of fantastic LWN articles).
  ul
    li #[a(href="https://www.akkadia.org/drepper/cpumemory.pdf") What every programmer should know about memory] (Drepper, 2007): A fantastic overview of memory in Linux. This interleaves useful knowledge (designed for the programmer) about the hardware with practical examples running on commodity and server hardware. It dives deep into many relevant topics surrounding memory, especially but not limited to: memory hardware, caching, virtualization/paging, CPU and memory subsystem topology (e.g., NUMA factors), hardware/software prefetching, effects of SMT/MT/SMP, TLB/page fault optimization, etc. It's 15 years old but still very relevant: see #[a(href="https://stackoverflow.com/a/47714514") this Stack Overflow post] for more details on its modern relevance.
    li #[a(href="https://www.hudsonrivertrading.com/hrtbeat/low-latency-optimization-part-1/") Low latency optimization: understanding huge pages] (Morin, 2022): This was what brought me to the Drepper paper in the first place, as I was exploring the high frequency trading space. After reading the Drepper paper, huge pages make sense for many reasons, but it wasn't recommended at that time (2007). But memory capacity has increased since then and high-frequency trading is extremely performance-critical, so this article shows that this optimization is very useful with practical experiments.

  h4 Memory consistency and cache coherence
  p I wanted to study these problems because they are very relevant in low-level programming in general. Referencing the definitions provided by the Sorin et al. paper listed below, we can define these problems roughly as: #[em consistency]: the correctness of shared memory and the (observed) order of loads and stores; #[em coherence]: the correctness of unshared caches. Coherence is generally required to have consistency, as (generally) all loads and stores go through the cache.
  p I encountered these issues at my time in Android because we dealt with an incoherent memory subsystem and ARM has a relaxed memory ordering. It's not too useful in my everyday programming given that x86_64 tends to be strongly ordered and many memory subsystems are coherent, but it's good to know and understand#[+footnote Sorin et al. make the distinction that memory consistency is important for the programmer to get right when dealing with shared memory and concurrency, whereas coherence is usually a problem abstracted away by the hardware.].
  p Memory consistency in particular was especially confusing to me: it didn't seem to make sense that different CPUs could see the same memory writes in different orders. But it turns out that there are many different observers of memory operations: "source code order," program order, execution order, and "observed order" (by other CPUs). Source code order may differ from program order (the compiled assembly) due to compiler optimizations; program order may differ from execution order due to out-of-order execution (pipelining) optimizations; and execution order may differ from the order that other CPUs view the memory due to second-order effects such as a cache store buffer (see #[a(href="https://stackoverflow.com/a/70749413/2397327") this Stack Overflow answer] for more details). These optimizations can be disabled by providing stricter memory ordering hints (i.e., #[code std::memory_order] hints to operations on #[code std::atomic]s, or directly calling memory fence instructions).
  ul
    li #[a(href="https://course.ece.cmu.edu/~ece847c/S15/lib/exe/fetch.php?media=part2_2_sorin12.pdf") A primer on memory consistency and cache coherence] (Sorin, Hill, and Wood, 2011): I haven't read the whole thing, but this provides a good outline on the titular problems.
    li #[a(href="https://www.youtube.com/watch?v=A_vAG6LIHwQ") Atomic's memory orders, what for?] (Birbachar, 2017): A practical walkthrough of different memory orders supported by the C++ #[code std::atomic&lt;T&gt;] standard. I.e., explains relaxed vs. release/acquire vs. sequential consistency memory orders#[+footnote My reductive rundown of memory orders: #[em relaxed] memory order doesn't guarantee anything; #[em release/acquire] semantics orders memory operations w.r.t. a single #[code std::atomic]; and #[em sequential consistency] #[a(href="https://stackoverflow.com/a/12340924/2397327") gives a total order for all #[code std::atomic]s] (that doesn't contradict the program order for any single thread).] with some practical examples. He also clearly explains the difference between sequence points, synchronization, and happens-before relationships, giving a pretty good overall view on synchronization (again, with practical examples in C++).

  h4 Synchronization (atomics and futexes)
  p This study of futexes was born out of a desire to learn more about how synchronization primitives and atomics (at least in C++) are developed. The very high-level summary of my findings is this: all synchronization primitives and atomics are based on atomic CPU instructions, especially Compare-And-Swap (CAS). (Note that this works by temporarily pausing the MESI protocol in the caches). #[code std::atomic] is directly built on top of this, as well as lock-less data structures.
  p When a thread may need to wait/sleep to synchronize, Linux provides the futex (Fast Userspace mutEXES) API#[+footnote Wherever you find an article detailing the motivation for futexes, the stated motivation for futexes is that they are faster than the previous POSIX mutex synchronization mechanism. This old mutex required a switch into kernel mode, which was expensive especially when the lock was uncontested and a single CAS operation would do. This explanation makes sense to me, but I can't really find any other information on these old POSIX mutexes for some reason; modern #[a(href="https://github.com/lattera/glibc/blob/master/nptl/pthread_mutex_lock.c#L168") POSIX mutexes seem to be based on futexes].], which is a generalization of (and is used to implement both) mutexes and condition variables. Note that futexes still use CAS to manage state atomically, and may transfer control to the kernel if the thread needs to block. Futexes also require a kernel waitqueue implementation that cause a quick wakeup for waiting threads upon signalling a futex.
  p Now, C++'s #[code std::atomic&lt;T&gt;] is actually more powerful than simply providing atomic access to their contents (of type #[code T]). It turns out that synchronization atomics are much more useful for synchronization if they also provide memory ordering semantics#[+footnote By default, the default memory ordering for #[code std::atomic] load/stores is sequential consistency (#[code std::memory_order::seq_cst]). If we only needed purely atomic behavior and didn't care about the relative memory ordering of other variables, then we can use #[code std::memory_order::relaxed].]; this is explained in more detail by Birbachar's talk linked above.
  ul
    li #[a(href="https://eli.thegreenplace.net/2018/basics-of-futexes/") Basics of futexes] (Bendersky, 2018): A brief overview to the futex API. It's probably good to also read the #[a(href="https://man7.org/linux/man-pages/man2/futex.2.html") futex (2)] manpage, but I haven't gotten through that yet.
    li #[a(href="https://dept-info.labri.fr/~denis/Enseignement/2008-IR/Articles/01-futex.pdf") Futexes are tricky] (Drepper, 2004): Referenced by the Bendersky article, this shows a basic implementation of a mutex using the futex interface. The implementation is indeed tricky.
    li #[a(href="https://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html") Double-Checked Locking is Broken] (Bacon et al., ~1999): While we're on the topic of locking, I wanted to read this paper recommended to me by my former tech lead at Google. This issue arises due to the #[a(href="https://en.wikipedia.org/wiki/Java_memory_model") JMM] and the pre-Java1.5 inability to add explicit memory fences or control memory order like in C++. Note that this issue is fixed after Java1.5 added the #[code volatile] keyword, which #[a(href="https://stackoverflow.com/a/13688818/2397327") provides a functionality similar to #[code std::atomic] with memory barriers by default]. Like the above Drepper paper, this is a case of a non-trivial synchronization bug. It acts as a great brain-teaser!
